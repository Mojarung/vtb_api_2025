spring:
  application:
    name: llm-service
  
  profiles:
    active: ${SPRING_PROFILES_ACTIVE:dev}
  
  data:
    redis:
      host: ${SPRING_REDIS_HOST:localhost}
      port: ${SPRING_REDIS_PORT:6379}
      password: ${SPRING_REDIS_PASSWORD:}
      timeout: 60000

server:
  port: ${SERVER_PORT:8082}

# Actuator
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics
  endpoint:
    health:
      show-details: always

# LLM Configuration
llm:
  # Nvidia API (Primary - ваш ключ)
  nvidia:
    api-key: ${NVIDIA_API_KEY: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}
    api-url: https://integrate.api.nvidia.com/v1/chat/completions
    model: qwen/qwen3-next-80b-a3b-instruct
    enabled: true
  
  # OpenAI (Optional fallback)
  openai:
    api-key: ${OPENAI_API_KEY:}
    model: gpt-4-turbo-preview
    enabled: ${OPENAI_ENABLED:false}
  
  # Anthropic Claude (Optional fallback)
  anthropic:
    api-key: ${ANTHROPIC_API_KEY:}
    model: claude-3-opus-20240229
    enabled: ${ANTHROPIC_ENABLED:false}
  
  # General settings
  primary-provider: ${LLM_PRIMARY_PROVIDER:nvidia}
  cache:
    enabled: true
    ttl: 86400
  timeout-seconds: 60
  max-tokens: 4096
  temperature: 0.2
  min-confidence: 0.7

# Logging
logging:
  level:
    root: INFO
    com.vtb.guardian: DEBUG
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"

---
# Production Profile
spring:
  config:
    activate:
      on-profile: prod

logging:
  level:
    root: WARN
    com.vtb.guardian: INFO

